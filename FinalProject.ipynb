{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project : Garbage bags classification\n",
    "\n",
    "| Course | Foundations of Machine Learning Frameworks |\n",
    "|--------|--------------------------------------------|\n",
    "| Professor | Ran Feldesh                             |\n",
    "| Group  | Group 10                                   |\n",
    "| Members | 8930180 - Burman, Jaiv Chaitanya <br>  9027375 - Lad, Khushbu Nileshkumar|        \n",
    "| Due date | 11th, December, 2024                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtain the Data: Get the Dogs vs Cats dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(see class' notebook, only 5000, not 25,000, images are required, these image set is defined in the notebook)\n",
    "1. Downloaded the dataset\n",
    "2. Added .data folder in repository and added it into gitignore.\n",
    "3. Extracted downloaded files into .data folder\n",
    "4. Path to data is \"./dogs-vs-cats/train\" and \"./dogs-vs-cats/test1\" \n",
    "5. Compress and divide train data into train, validation and test set as per class notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dir = pathlib.Path(\".data/Waste-Classification/Bag Classes/\")\n",
    "new_base_dir = pathlib.Path(\".data/Waste-Classification/Subsets/\")\n",
    "new_base_dir_train = pathlib.Path(\".data/Waste-Classification/Subsets/train/\")\n",
    "new_base_dir_validation = pathlib.Path(\".data/Waste-Classification/Subsets/validation/\")\n",
    "new_base_dir_test = pathlib.Path(\".data/Waste-Classification/Subsets/test/\")\n",
    "labels = [\"Garbage Bag Images\",\"Paper Bag Images\",\"Plastic Bag Images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in labels:\n",
    "        from_dir = original_dir / category\n",
    "        to_dir = new_base_dir / subset_name / category\n",
    "        os.makedirs(to_dir)\n",
    "        files = [f for f in os.listdir(from_dir) if os.path.isfile(os.path.join(from_dir, f))]\n",
    "        for i, fname in enumerate(files):\n",
    "            if start_index <= i < end_index:\n",
    "                shutil.copyfile(src=from_dir / fname, dst=to_dir / fname)\n",
    "                            \n",
    "\n",
    "make_subset(\"train\", start_index=0, end_index=2000)             # 2000\n",
    "make_subset(\"validation\", start_index=2000, end_index=3000)     # 1000\n",
    "make_subset(\"test\", start_index=3000, end_index=5000)           # 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. EDA: Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/Garbage Bag Images: 2000 images\n",
      "train/Paper Bag Images: 2000 images\n",
      "train/Plastic Bag Images: 2000 images\n",
      "validation/Garbage Bag Images: 1000 images\n",
      "validation/Paper Bag Images: 1000 images\n",
      "validation/Plastic Bag Images: 1000 images\n",
      "test/Garbage Bag Images: 2000 images\n",
      "test/Paper Bag Images: 2000 images\n",
      "test/Plastic Bag Images: 2000 images\n"
     ]
    }
   ],
   "source": [
    "for subset in [\"train\", \"validation\", \"test\"]:\n",
    "    for category in labels:\n",
    "        dir_path = new_base_dir / subset / category\n",
    "        print(f\"{subset}/{category}: {len(os.listdir(dir_path))} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_images(dataset_dir, num_samples=5):\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
    "\n",
    "    # Get all files from the directory\n",
    "    files = [f for f in os.listdir(dataset_dir) if os.path.isfile(os.path.join(dataset_dir, f))]\n",
    "\n",
    "    # Randomly select 5 files\n",
    "    random_files = random.sample(files, 5)\n",
    "\n",
    "    print(\"Random Files:\", random_files)\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        label = label[i % 3]\n",
    "        img_path = os.path.join(dataset_dir, label, f\"{label}.{i}.jpg\")\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"A sample {label}\")\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "   display_sample_images(new_base_dir_train,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create train, validation and test dataset using tansorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6000 files belonging to 3 classes.\n",
      "Found 3000 files belonging to 3 classes.\n",
      "Found 6000 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create train, validation and test dataset using tansorflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvCSCN8010",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
